## ONNX Model Optimization

### 1. Background Introduction

In text recognition models, such as CRNN, the final layer is a softmax layer. In the post process, the argmax function is used to calculate the index of  character recognized.

On one hand, for the purpose of text recognition, the probabilities generated by the softmax are not needed. Therefore, it is considered to remove the softmax node from the ONNX model in order to accelerate the inference process.

On the other hand, the numpy argmax function used in the post process is relatively time-consuming compared to the argmax operator used in the model. Hence, it is considered inserting an argmax operator into the ONNX model to replace the numpy argmax function in post process.

### 2. Optimization Approach

The structure of the original ONNX model before optimization is shown below:

<p align="center">
  <img src="https://user-images.githubusercontent.com/122354463/250898682-3a15ec8b-9c96-4582-877e-e843ea3dcffd.png" width=480 />
</p>
<p align="center">
  <em>Before Optimization</em>
</p>

run script [insert_argmax.py](../../../deploy/models_utils/onnx_optim/insert_argmax.py). Pass the parameter model_path as the file path of the ONNX network to be optimizedã€‚

```python
python deploy/models_utils/onnx_optim/insert_argmax.py --model_path {path_to_model}
```
By deleting the softmax node and inserting an argmax node, the optimized model structure becomes as follows:

<p align="center">
  <img src="https://user-images.githubusercontent.com/122354463/250901029-c95a3120-b36b-486c-8952-97dcb3ab0ec8.png" width=480 />
</p>
<p align="center">
  <em>After Optimization</em>
</p>

Through testing, when the input batch_size is 16, taking the ch_ppocr_server_v2.0_rec_infer.onnx model as an example, the inference time is reduced by 64.4% when using the Ascend 310.



